{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Người dân trồng dừa, bưởi được hướng dẫn áp dụng quy trình kỹ thuật trồng và chăm sóc VietGAP.\n",
      "Segmented: Người_dân trồng dừa , bưởi được hướng_dẫn áp_dụng quy_trình kỹ_thuật trồng và chăm_sóc VietGAP .\n",
      "\n",
      "Sentence: Tuyến đường này giúp rút ngắn cự ly từ Đà Nẵng đến TP Tam Kỳ (Quảng Nam) khoảng 10 km, giảm áp lực giao thông cho quốc lộ 1.\n",
      "Segmented: Tuyến đường này giúp rút ngắn_cự_ly từ Đà_Nẵng đến TP Tam_Kỳ ( Quảng_Nam ) khoảng 10 km , giảm áp_lực giao_thông cho quốc_lộ 1 .\n"
     ]
    }
   ],
   "source": [
    "from underthesea import word_tokenize\n",
    "text1 = 'Người dân trồng dừa, bưởi được hướng dẫn áp dụng quy trình kỹ thuật trồng và chăm sóc VietGAP.'\n",
    "text2 = \"Tuyến đường này giúp rút ngắn cự ly từ Đà Nẵng đến TP Tam Kỳ (Quảng Nam) khoảng 10 km, giảm áp lực giao thông cho quốc lộ 1.\"\n",
    "\n",
    "seg1 = word_tokenize(text1, format=\"text\")\n",
    "seg2 = word_tokenize(text2, format=\"text\")\n",
    "\n",
    "print(\"Sentence: \" + text1)\n",
    "print(\"Segmented: \" + seg1)\n",
    "print()\n",
    "print(\"Sentence: \" + text2)\n",
    "print(\"Segmented: \" + seg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Con chó của tôi rất dễ thương\n",
      "Segmented: Con chó của tôi rất dễ_thương\n",
      "\n",
      "Sentence: Giống chó bẹc-giê thường được dùng làm chó nghiệp vụ\n",
      "Segmented: Giống chó bẹc-giê thường được dùng làm chó nghiệp_vụ\n"
     ]
    }
   ],
   "source": [
    "from underthesea import word_tokenize\n",
    "text1 = 'Con chó của tôi rất dễ thương'\n",
    "text2 = 'Giống chó bẹc-giê thường được dùng làm chó nghiệp vụ'\n",
    "\n",
    "seg1 = word_tokenize(text1, format=\"text\")\n",
    "seg2 = word_tokenize(text2, format=\"text\")\n",
    "\n",
    "print(\"Sentence: \" + text1)\n",
    "print(\"Segmented: \" + seg1)\n",
    "print()\n",
    "print(\"Sentence: \" + text2)\n",
    "print(\"Segmented: \" + seg2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
